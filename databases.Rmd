Database/SQL Workshop
======================================================================================
January 2021, UC Berkeley Statistical Computing Facility
----------------------------------------------------------

Chris Paciorek, Department of Statistics, UC Berkeley

```{r setup, include=FALSE}
options(replace.assign=TRUE, width=65)
opts_chunk$set(eval = FALSE, message = FALSE) ## turned off message() output
library(knitr)
read_chunk('examples.R')
library(DBI)
SLOWEVAL <- TRUE
```


# 0) Introduction

This workshop will cover using and administering databases (SQLite and PostgreSQL) and making SQL queries of various complexity (from R and Python).

I'll do a bunch of demo with time for you to work on individual questions as well as time to compare and contrast different people's answers.

This workshop is based essentially completely on [this SCF tutorial](https://github.com/berkeley-scf/tutorial-databases).

At times I'll ask you to post code/syntax on this [shared document](https://docs.google.com/document/d/1Wnlw0NKlUzPTA7v0S704782RI3Q_gxdCamolK40qSU4/edit?usp=sharing).

## 0.1) Getting set up

This workshop assumes you have a working knowledge of R or Python. 

The example data files are not part of the Github repository. You can get the example SQLite database [here](http://www.stat.berkeley.edu/share/paciorek/stackoverflow-2016.db).

You can get the input data files we'll work with when setting up an example database [here (UNDER CONSTRUCTION)](todo).

Solutions to the SQL challenges are available on request. 

# 1) Review of database concepts

## 1.1) Overview of databases

Basically, standard SQL databases are *relational* databases that are a collection of rectangular format datasets (*tables*, also called *relations*), with each table similar to R or Pandas data frames, in that a table is made up of columns, which are called *fields* or *attributes*, each containing a single *type* (numeric, character, date, currency, enumerated (i.e., categorical), ...) and rows or records containing the observations for one entity. Some of these tables generally have fields in common so it makes sense to merge (i.e., join) information from multiple tables. E.g., you might have a database with a table of student information, a table of teacher information and a table of school information.

One principle of databases is that if a set of fields contain duplicated information about a given category, you can more efficiently store information about each level of the category in a separate table. Consider information about people living in a state and information about each state - you don't want to include variables that only vary by state in the table containing information about individuals (at least until you're doing the actual analysis that needs the information in a single table). Or consider students nested within classes nested within schools.

Databases are set up to allow for fast querying and merging (called joins in database terminology). 

You can interact with databases in a variety of database systems (DBMS=database management system). Some popular systems are SQLite, MySQL, PostgreSQL, Oracle and Microsoft Access. We'll concentrate on accessing data in a database rather than management of databases. SQL is the Structured Query Language and is a special-purpose high-level language for managing databases and making queries. Variations on SQL are used in many different DBMS.

Queries are the way that the user gets information (often simply subsets of tables or information merged across tables). The result of an SQL query is in general another table, though in some cases it might have only one row and/or one column.

Many DBMS have a client-server model. Clients connect to the server, with some authentication, and make requests (i.e., queries).

There are often multiple ways to interact with a DBMS, including directly using command line tools provided by the DBMS or via Python or R, among others. 

## 1.2) Relational Database Management Systems (DBMS)

There are a variety of relational database management systems (DBMS). Some that are commonly used by the intended audience of this tutorial are SQLite, PostgreSQL, and mySQL. We'll concentrate on SQLite (because it is simple to use on a single machine) and PostgreSQL (because is is a popular open-source DBMS that is a good representative of a client-server model and has some functionality that SQLite lacks).

SQLite is quite nice in terms of being self-contained - there is no server-client model, just a single file on your hard drive that stores the database and to which you can connect to using the SQLite shell, R, Python, etc.  However, it does not have some useful functionality that other DBMS have. For example, you can't use `ALTER TABLE` to modify column types or drop columns. 

## 1.3) Schema and normalization

To truly leverage the conceptual and computational power of a database you'll want to have your data in a normalized form, which means spreading your data across multiple tables in such a way that you don't repeat information unnecessarily.

The schema is the metadata about the tables in the database and the fields (and their types) in those tables.

### 1.3.1) Example

The database we'll work with as our example is meta-data on questions and answers on the Stack Overflow code discussion website from 2016. We don't have the full text of the questions or the answers, but we do have:

- the question and answer titles,
- some information about how popular the questions and answers were,
- information about the users posting questions or answers, and
- the topics (tags) associated with each question.

Each question may have zero or more answers and each question may have one or more tags indicating what the question is above.

Let's consider how we might lay out the rows and columns of the Stack Overflow database if we tried to do it all in one table (e.g., all in one spreadsheet), with one row per question:

- question id
- question title
- date
- viewcount
- owner id
- owner_age
- owner_displayname
- answer 1 title
- ...
- answer n title
- answerer 1 id
- ...
- answerer n id
- answerer 1 name
- ...
- answerer n name
- answerer 1 age
- ...
- answerer n age


An alternative would be like this, with one row per question-answer pair:

- question id
- question title
- date
- viewcount
- answer title
- answerer id
- answerer name
- answerer age

***Challenge***: These approaches have a lot of disadvantages. Let's list them as a group:

  - ??
  - ??
  - ??

Furthermore, suppose we wanted the tags included in our gigantic table. We could add tags in either with mutiple columns or with one row per question-answer-tag triplet. The problems remain the same, but are even worse ...

Those disadvantages lead to the idea of normalizing the data. 

You can view one [reasonable schema here](normalized_example.png) that shows how the data are laid out into separate tables for questions, answers, users, and tags assigned to questions. The schema in the actual databases of Stack Overflow data we'll use in this tutorial is similar to but not identical to that. 

The basic idea is to think about what tables you would create such that:

- each row contains unique information,
- there is no duplication of information in some other row, and
- the number of columns can be known in advance without having columns with blank entries.

(That said, if all anticipated uses of a database will end up recombining the same set of tables, we may want to have a denormalized schema in which those tables are actually combined in the database. It is possible to be too pure about normalization! We can also create a virtual table, called a *view*, as discussed later.)

### 1.3.2) Keys

A key is one (or occasionally more) fields that give a unique value for every row/observation. A table in a database should then have a primary key that is the main unique identifier used by the DBMS. Foreign keys are columns in one table that give the value of the primary key in another table. When information from multiple tables is joined together, the matching of a row from one table to a row in another table is generally done by equating the primary key in one table with a foreign key in a different table.

The lines between tables indicate the relationship of foreign keys in one table to primary keys in another table in the [schema we just saw](normalized_example.png).

In our Stack Overflow example, the primary keys would presumably be: questions.questionid, answers.answerid, users.userid, and for QuestionsTags table two fields: {QuestionsTags.questionid, QuestionsTags.tag}.

Some examples of foreign keys would be:
 - ownerid as the foreign key in Questions for joining on the userid primary key in Users
 - questionid as the foreign key in Answers for joining on the questionid primary key in Questions

# 2) Accessing a database from R or Python

## 2.1) Getting the example database

I've obtained data from [Stack Overflow](https://stackoverflow.com), the popular website for asking coding questions, and placed it into a normalized database. The SQLite version (also in CSVs as one CSV per table) has metadata (i.e., it lacks the actual text of the questions and answers) on all of the questions and answers posted in 2016.

You can download a copy of the SQLite version of the Stack Overflow database (only data for the year 2016) from [here](http://www.stat.berkeley.edu/share/paciorek/stackoverflow-2016.db) as discussed in the introduction of this tutorial. 

Note that all of the code used to download the data from the Stack Overflow website and to manipulate it to create a complete Postgres database and (for the year 2016 only) an SQLite database and CSVs for each table is in the `data/prep_stackoverflow` subdirectory of [this repository](https://github.com/berkeley-scf/tutorial-databases). Note that as of January 2020, [the data are still being kept up to date online](https://archive.org/download/stackexchange).

Although DBMS have their own interfaces (we'll see a bit of this later), databases are commonly accessed from other programs. For data analysts this would often be Python or R, as seen next.

Most of our examples of making SQL queries on a database will be done from R, but they could just as easily have been done from Python or other programs.

## 2.2) Using SQL from R

The *DBI* package provides a front-end for manipulating databases from a variety of DBMS (SQLite, MySQL, PostgreSQL, among others).
Basically, you

1) tell the package what DBMS is being used on the back-end,
2) link to the actual database, and then
3) use the standard functions in the package **regardless** of the back-end.

With SQLite, R processes make calls against the stand-alone SQLite database (.db) file, so there are no SQLite-specific processes. With PostgreSQL, R processes call out to separate Postgres processes; these are started from the overall Postgres background process

You can access and navigate an SQLite database from R as follows.

```{r, eval=TRUE}
library(RSQLite)
drv <- dbDriver("SQLite")
dir <- '.' # relative or absolute path to where the .db file is; here just the working directory
dbFilename <- 'stackoverflow-2016.db'
db <- dbConnect(drv, dbname = file.path(dir, dbFilename))
dbGetQuery(db, "select * from questions limit 5")  # simple query to get 5 rows from a table
```

We can easily see the tables and their fields:
```{r, eval=TRUE}
dbListTables(db)
dbListFields(db, "questions")
dbListFields(db, "answers")
```

One can either make the query and get the results in one go or make the query and separately fetch the results. Here we've selected the first five rows (and all columns, based on the `*` wildcard) and brought them into R as a data frame.


```{r, eval=TRUE}
results <- dbGetQuery(db, 'select * from questions limit 5')
class(results)

query <- dbSendQuery(db, "select * from questions")
query
results2 <- fetch(query, 5)
identical(results, results2)
dbClearResult(query)  # clear to prepare for another query
```


To disconnect from the database (you probably don't want to do this now...):

```{r, eval=FALSE}
dbDisconnect(db)
```

To access a PostgreSQL database instead, you can do the following, assuming the database has been created and you have a username and password that allow you to access the particular database.

```{r, eval=FALSE}
library(RPostgreSQL)
drv <- dbDriver("PostgreSQL")
db <- dbConnect(drv, dbname = 'stackoverflow', user = 'paciorek', password = 'test')
```

Apart from the different manner of connecting, all of the queries above are the same regardless of whether the back-end DBMS is SQLite, PostgreSQL, etc.

## 2.3) Using SQL from Python

For SQLite:

```{python, eval=FALSE}
import sqlite3 as sq
dir <- '.' # relative or absolute path to where the .db file is
dbFilename <- 'stackoverflow-2016.db'
import os
db = sq.connect(os.path.join('data', dbFilename))
c = db.cursor()
c.execute("select * from questions limit 5")  # simple query 
results = c.fetchall() # retrieve results
```

To disconnect:
```{python, eval=FALSE}
c.close()
```

Here's how you would connect to PostgreSQL instead:

```{python, eval=FALSE}
import psycopg2 as pg
db = pg.connect("dbname = 'stackoverflow' user = 'paciorek' host = 'localhost' password = 'test'")
c = db.cursor()
```

# 3) Using SQL

## 3.1) Review of basic SQL syntax

SQL is a declarative language that tells the database system what results you want. The system then parses the SQL syntax and determines how to implement the query.

We've already seen the simple query that selects the first five rows (and all columns, based on the `*` wildcard) from the questions table. 

```
select * from questions limit 5
```

Let's lay out the various verbs in SQL. Here's the form of a standard query (though the ORDER BY is often not used and sorting is computationally expensive):

```
SELECT <column(s)> FROM <table> WHERE <condition(s) on column(s)> ORDER BY <column(s)>
```

SQL keywords are often written in ALL CAPITALS though I won't necessarily do that in this tutorial. 

And here is a table of some important keywords:


|  Keyword          | What it does  
|------------------------|------------------------
| SELECT                  | select columns 
| FROM                    | which table to operate on
| WHERE                   | filter (choose) rows satisfying certain conditions
| LIKE, IN, <, >, =, etc. | used as part of conditions
| ORDER BY                | sort based on columns

For comparisons in a WHERE clause, some common syntax for setting conditions includes LIKE (for patterns), =, >, <, >=, <=, !=.

Some other keywords are: DISTINCT, ON, JOIN, GROUP BY, AS, USING, UNION, INTERSECT, SIMILAR TO, SUBSTR in SQLite and SUBSTRING in PostgreSQL. 


## 3.2) Simple joins

The syntax generally looks like this (again the WHERE and ORDER BY are optional):

```
SELECT <column(s)> FROM <table1> JOIN <table2> ON <columns to match on>
   WHERE <condition(s) on column(s)> ORDER BY <column(s)>
```

Let's consider trying to create a giant table that has all (or most of) the information in the various tables. This might be how we would have the data if we tried to set it up as a single spreadsheet.


```{r, eval=SLOWEVAL}
## a join with JOIN
result1 <- dbGetQuery(db, "select * from questions join answers
           on questions.questionid = answers.questionid")

## a join without JOIN
result2 <- dbGetQuery(db, "select * from questions, answers
        where questions.questionid = answers.questionid")
head(result2)

identical(result1, result2)
```

That would create a table with one row per question-answer pair.

Here's a three-way join with some additional use of aliases to abbreviate table names. What does this query do?

```{r, eval=SLOWEVAL}
## with JOIN
result1 <- dbGetQuery(db, "select * from questions Q
        join answers A on Q.questionid = A.questionid
        join users U on Q.ownerid = U.userid")

## without JOIN
result2 <- dbGetQuery(db, "select * from questions Q, answers A, users U
        where Q.questionid = A.questionid 
          and Q.ownerid = U.userid")
          
identical(result1, result2)
```

As before, we can see the disadvantages of having a single table.

***Challenge***: What information is missing from joining questions, answers, and users in this way? What kinds of questions and what kinds of users?

## 3.3) Grouping / stratifying

A common pattern of operation is to stratify the dataset, i.e., collect it into mutually exclusive and exhaustive subsets. One would then generally do some operation on each subset. In SQL this is done with the GROUP BY keyword.

Here's a basic example where we count the occurrences of different tags. 

```{r, eval=FALSE}
dbGetQuery(db, "select tag, count(*) as n from questions_tags
                group by tag order by n desc limit 20")
```

***Challenge***: What specifically does that query do? Describe the table that would be returned.

In general `GROUP BY` statements will involve some aggregation operation on the subsets. Options include: COUNT, MIN, MAX, AVG, SUM.

Note that to filter the result of a grouping operation, we would need to use `having` rather than `where`.

Also note the use of `as` to define a name for the new column.


```{r, eval=TRUE}
dbGetQuery(db, "select tag, count(*) as n from questions_tags
                group by tag having n > 100000 limit 10")
```

## 3.4) Getting unique results (DISTINCT)

A useful SQL keyword is DISTINCT, which allows you to eliminate duplicate rows from any table (or remove duplicate values when one only has a single column or set of values).

```{r, eval=TRUE}
tagNames <- dbGetQuery(db, "select distinct tag from questions_tags")
dbGetQuery(db, "select count(distinct tag) from questions_tags")
```

## 3.5) Creating database tables

One can create tables from within the `sqlite` and `psql` command line interfaces (discussed later), but often one would do this from R or Python. Here's the syntax from R.

```{r, eval=FALSE}
## Option 1: pass directly from CSV to database
dbWriteTable(conn = db, name = "student", value = "student.csv", row.names = FALSE, header = TRUE)

## Option 2: pass from data in an R data frame
## First create your data frame:
# student <- data.frame(...)
## or
# student <- read.csv(...)
dbWriteTable(conn = db, name = "student", value = student, row.names = FALSE, append = FALSE)
```

## 3.6) Building up an example query

Let's see an example of building up a (somewhat) more complicated query in pieces, using the tools discussed above.

1. First suppose we want to find the number of users who have asked questions.

```{r, eval=FALSE, echo=FALSE}

```

2. How about finding the users who have asked at least 50 questions? How do we determine the number of questions per user? What are we grouping by?

```{r, eval=FALSE, echo=FALSE}

```

3. What if we wanted actual information about the users, not just their ID? We need information from the users table.

```{r, eval=FALSE, echo=FALSE}

```

4. ***Challenge***: What if we wanted to count the number of users who have asked at least 50 questions? We'll discuss one way to do this later.

# 4) More advanced SQL

## 4.1) More on joins

We've seen a bunch of joins but haven't discussed the full taxonomy of types of joins. There are various possibilities for how to do a join depending on whether there are rows in one table that do not match any rows in another table.

## 4.1.1) Types of joins

*Inner joins*: In database terminology an inner join is when the result has a row for each match of a row in one table with the rows in the second table, where the matching is done on the columns you indicate. If a row in one table corresponds to more than one row in another table, you get all of the matching rows in the second table, with the information from the first table duplicated for each of the resulting rows. For example in the Stack Overflow data, an inner join of questions and answers would pair each question with each of the answers to that question. However, questions without any answers or (if this were possible) answers without a corresponding question would not be part of the result.

*Outer joins*: Outer joins add additional rows from one table that do not match any rows from the other table as follows. A *left outer join* gives all the rows from the first table but only those from the second table that match a row in the first table. A *right outer join* is the converse, while a *full outer join* includes at least one copy of all rows from both tables. So a left outer join of the Stack Overflow questions and answers tables would, in addition to the matched questions and their answers, include a row for each question without any answers, as would a full outer join. In this case there should be no answers that do not correspond to question, so a right outer join should be the same as an inner join. 

*Cross joins*: A cross join gives the Cartesian product of the two tables, namely the pairwise combination of every row from each table, analogous to `expand.grid` in R. I.e., take a row from the first table and pair it with each row from the second table, then repeat that for all rows from the first table. Since cross joins pair each row in one table with all the rows in another table, the resulting table can be quite large (the product of the number of rows in the two tables). In the Stack Overflow database, a cross join would pair each question with every answer in the database, regardless of whether the answer is an answer to that question.


Here's a table of the different kinds of joins:

|  Type of join          |   Rows from first table  | Rows from second table
|------------------------|--------------------------|---------------------------------------------
| inner (default)       |   all that match on specified condition        | all that match on specified condition
| left outer             |   all                    | all that match first
| right outer            |   all that match second | all 
| full outer             |   all                    | all
| cross                  |   all combined pairwise with second | all combined pairwise with first

Let's consider a very basic example. Suppose we have a table of Stack Overflow users with a column for the country they are from. And suppose we have a table with country information, such as country population, GDP, etc. In the workshop, we'll consider different kinds of joins with such tables.

### 4.1.2) More on joins

A 'natural' join is an inner join that doesn't require you to specify the common columns between tables on which to enforce equality, but it's often good practice to not use a natural join and to explicitly indicate which columns are being matched on.

Simply listing two or more tables separated by commas as we saw earlier is the same as a *cross join*. Alternatively, listing two or more tables separated by commas, followed by conditions that equate rows in one table to rows in another is the same as an *inner join*. 

In general, inner joins can be seen as a form of cross join followed by a condition that enforces matching between the rows of the table. More broadly, here are five equivalent joins that all perform the equivalent of an inner join:

```
select * from table1 join table2 on table1.id = table2.id ## explicit inner join
select * from table1, table2 where table1.id = table2.id  ## without explicit (cross join)
select * from table1 cross join table2 where table1.id = table2.id  ## explicit cross join
select * from table1 join table2 using(id)                ## inner join again
select * from table1 natural join table2                  ## natural (inner) join
```

Note that in the last query the join would be based on all common columns, which could be a bit dangerous if you don't look carefully at the schema of both tables. Assuming `id` is the common column, then the last of these queries is the same as the others.

***Challenge***:  Suppose the Stack Overflow database has 10,000 questions, 30,000 answers, and 5,000 users. Please answer in the Zoom poll.

1. If you do an inner join of questions and answers, how many rows would the result have?
2. If you do a left outer join of questions and answers, how many rows would the result have?
3. If you do a right outer join of questions and answers, how many rows would the result have?
4. If you do a right outer join of questions and *users*, how many rows would the result have?
5. If you do a cross join of questions and answers, how many rows would the result have?

***Challenge***: What question would item #4 allow you to figure out?

## 4.2) Joining a table with itself (self joins)

Sometimes we want to query information across rows of the same table. For example supposed we want to analyze the time lags between when the same person posts a question. Do people tend to post in bursts or do they tend to post uniformly over the year? To do this we need contrasts between the times of the different posts. (One can also address this using window functions, discussed later.)

So we need to join two copies of the same table, which means dealing with resolving the multiple copies of each column.

This would look like this:

```{r, eval=FALSE}
dbGetQuery(db, "create view question_contrasts as
               select * from questions Q1 join questions Q2
               on Q1.ownerid = Q2.ownerid")
```

***Challenge***: Suppose for simplicity that the database consists of three users, one of whom has asked two questions, one has asked three questions, and one has asked four questions.
How many rows are in the result?

Actually, there's a problem here.

***Challenge***: How many rows do we really want? What kinds of rows will we get that we don't want? 

A solution to that problem is:

```{r, selfjoin, eval=FALSE, echo=FALSE}
```

***Challenge***: There's actually a further similar problem. What is the problem and how can we fix it by changing two characters in the query above? Hint, even as character strings, the creationdate column has an ordering.


## 4.3) Set operations: UNION, INTERSECT, EXCEPT

You can do set operations like union, intersection, and set difference using the UNION, INTERSECT, and EXCEPT keywords on tables that have the same schema (same column names and types), though most often these would be used on single columns (i.e., single-column tables).

Note that one can often set up an equivalent query without using INTERSECT or UNION.

Here's an example of a query that can be done with or without an intersection. Suppose we want to know the names of all individuals who have asked both an R question and a Python question. We can do this with INTERSECT:


```{r, eval=SLOWEVAL}
system.time(
        result1 <- dbGetQuery(db, "select displayname, userid from
               questions Q join users U on U.userid = Q.ownerid
               join questions_tags T on Q.questionid = T.questionid
               where tag = 'r'
               intersect
               select displayname, userid from
               questions Q join users U on U.userid = Q.ownerid
               join questions_tags T on Q.questionid = T.questionid
               where tag = 'python'")
               )
```

Alternatively we can do a self-join. Note that the syntax gets complicated as we are doing multiple joins.

```{r, eval=SLOWEVAL}
system.time(
        result2 <- dbGetQuery(db, "select displayname, userid from
               (questions Q1 join questions_tags T1
               on Q1.questionid = T1.questionid)
               join
               (questions Q2 join questions_tags T2
               on Q2.questionid = T2.questionid)
               on Q1.ownerid = Q2.ownerid
               join users on Q1.ownerid = users.userid
               where T1.tag = 'r' and T2.tag = 'python'")
               )
identical(result1, result2)
```

Note that the second query will return duplicates where we have a person asking multiple R or Python queries. But we know how to solve that by including a DISTINCT:

```
select distinct displayname, userid from ...
``` 

Which is faster? The second one looks more involved in terms of the joins, so the timing results seen above make sense.


Or we could use UNION or EXCEPT to find people who have asked either or only one type of question, respectively.

***Challenge***: Find all the questions about either R or Python.

## 4.4) String processing and creating new fields

We can do some basic matching with LIKE, using % as a wildcard and _ to stand in for any single character:

```{r, eval=TRUE}
dbGetQuery(db, "select * from questions_tags where tag like 'r-%' limit 10")
```

In Postgres, in addition the basic use of LIKE to match character strings, one can regular expression syntax with SIMILAR TO and one can extract substrings with SUBSTRING.

These keywords are not available in SQLite so the following can only be done in the Postgres instance of our example database. Here we'll look for all tags that are of the form "r-", "-r", "r" or "-r-". SQL uses % as a wildcard (this is not standard regular expression syntax). 


```{r, eval=FALSE}
## Try in postgreSQL, not SQLite
result <- dbGetQuery(db, "select * from questions_tags where tag SIMILAR TO 'r-%|%-r|r|%-r-%' limit 10")
```

To extract substrings we use SUBSTRING. Postgres requires that the pattern to be extracted be surrounded by `#"` (one could use another character in place of `#`), but for use from R we need to escape the double-quote with a backslash so it is treated as a part of the string passed to Postgres and not treated by R as indicating where the character string stops/starts. 

```{r, eval=FALSE}
## Try in postgreSQL, not SQLite
dbGetQuery(db, "select substring(creationdate from '#\"[[:digit:]]{4}#\"%' for '#') as year
               from questions limit 10")
```

Note that SQLite provides SUBSTR for substrings, but the flexibility of SUBSTR seems to be much less than use of SUBSTRING in PostgreSQL.

Here is some [documentation on string functions in PostgreSQL](https://www.postgresql.org/docs/current/functions-string.html).

***Challenge***: Select the questions that have "java" in their titles using regular expression syntax.

***Challenge***: Figure out how to calculate the length (in characters) of the title of each question. 

***Challenge***:Process the creationdate field to create year, day, and month fields in a new view. Note that this would be good practice for string manipulation, but you would want to handle dates and times using the material in the next section and not use string processing.

## 4.5) Dates and times

Here we'll see how you can work with dates and times in SQLite, but the functionality should be similar in other DBMS.

SQLite doesn't have specific date-time types, but it's standard to store date-times as strings in the text field 
in the ISO-8601 format: YYYY-MM-DD HH:MM:SS.SSS. That's the format of the dates in the StackOverflow database:

```{r, eval=TRUE}
dbGetQuery(db, "select distinct creationdate from questions limit 5")
```

Then SQLite provides some powerful functions for manipulating and extracting information in such fields. Here are just a few examples, noting that `strftime` is particularly powerful. Other DBMS should have similar functionality, but I haven't investigated further. 


```{r, eval=TRUE}
## Julian days (decimal days since noon UTC/Greenwich time November 24, 4714 BC (Yikes!)). 
output <- dbGetQuery(db, "select creationdate, julianday(creationdate)
                from questions limit 5")
output
## Julian day is decimal-valued:
formatC(output[ , 2], 6, format = 'f')

## Convert to local time
dbGetQuery(db, "select distinct creationdate, datetime(creationdate, 'localtime')
                from questions limit 5")
## Eastern time, manually, ignoring daylight savings
dbGetQuery(db, "select distinct creationdate, datetime(creationdate, '-05:00')
                from questions limit 5")

## day of week: Jan 1 2016 was a Friday (0=Sunday, 6=Saturday)
dbGetQuery(db, "select creationdate, strftime('%w', creationdate)
                from questions limit 5")
```

Unfortunately I'm not sure if the actual dates in the database are Greenwich time or some US time zone, but we'll ignore that complication here.

Let's put it all together to do something meaningful.


```{r, eval=TRUE, fig.cap="", fig.width=5, fig.height=4}
result <- dbGetQuery(db, "select strftime('%H', creationdate) as hour,
                          count() as n from questions group by hour")
head(result)
plot(as.numeric(result$hour), result$n, xlab = 'hour of day (UTC/Greenwich???)',
                                        ylab = 'number of questions')
```

Here's some [documentation of the syntax for the functions, including `stftime`](https://www.sqlite.org/lang_datefunc.html).


## 4.6) Temporary tables and views

You can think of a view as a temporary table that is the result of a query and can be used in subsequent queries. In any given query you can use both views and tables. The advantage is that they provide modularity in our querying. For example, if a given operation (portion of a query) is needed repeatedly, one could abstract that as a view and then make use of that view.

Suppose we always want the age and displayname of question owners available. Once we have the view we can query it like a regular table.


```{r, eval=SLOWEVAL}
## note there is a creationdate in users too, hence disambiguation
dbExecute(db, "create view questionsAugment as
               select questionid, questions.creationdate, score, viewcount, title, ownerid, age, displayname
               from questions join users on questions.ownerid = users.userid")
## don't be confused by the "0" response --
## it just means that nothing is returned to R; the view _has_ been created
               
dbGetQuery(db, "select * from questionsAugment where age > 70 limit 5")
```
               
```{r, eval=TRUE, include=FALSE}
dbExecute(db, "drop view questionsAugment") # drop so can create again when rerun the code above
```

## 4.7) Subqueries

### 4.7.1) Subqueries in the FROM statement

We can use subqueries in the FROM statement to create a temporary table to use in a query. This is an alternative to using a view. Here we'll do it in the context of a join.

***Challenge***: What does the following do?


```{r, eval=FALSE}
dbGetQuery(db, "select T.tag, count(*) as n
               from questions_tags T join
               (select questionid from answers group by questionid
               order by count(*) desc limit 100) most_answered
               on T.questionid = most_answered.questionid
               group by T.tag
               order by n desc")
```

It might be hard to write that query directly from your brain. A good strategy is probably to think about creating a view that is the result of the inner query and then have the outer query use that. You can then piece together the complicated query in a modular way. For big databases, you are likely to want to submit this as a single query and not two queries so that the SQL optimizer can determine the best way to do the operations. But you want to start with code that you're confident will give you the right answer!

Note we could also have done that query using a subquery in the WHERE statement. 

***Challenge***: How would you write a query using join and group by to find the answer to each question from the user with the maximum reputation amongst all those answering the question?  Then use the result of as the subquery in a join with the questions table. The result should be the question information plus the title of the answer to that question written by the user with the highest reputation.



### 4.7.2) Subqueries in the WHERE statement

Instead of a join, we can use subqueries as a way to combine information across tables, with the subquery involved in a WHERE statement. The subquery creates a set and we then can check for inclusion in (or exclusion from with `not in`) that set.

For example, suppose we want to know the average number of UpVotes for users who have posted a question with the tag "python".

```{r, eval=TRUE}
dbGetQuery(db, "select avg(UpVotes) from users where userid in
               (select distinct ownerid from
               questions join questions_tags on questions.questionid = questions_tags.questionid
               where tag = 'python')")       
```

In some cases one can do a join rather than using a subquery, but in this example, it fails.

***Challenge***: What's wrong with the following query as an attempt to answer the question above? (See if you can figure it out before looking at the answer below.)

```{r, eval=FALSE}
dbGetQuery(db, "select avg(UpVotes) from questions, questions_tags, users
               where questions.questionid = questions_tags.questionid and
               questions.ownerid = users.userid and
               tag = 'python'")
```

For more details on subqueries, see the video on "subqueries in where statements" in this [Introduction to Databases MOOC](http://cs.stanford.edu/people/widom/DB-mooc.html).

(Answer: In the subquery, we find the Ids of the users we are looking for and then average over the UpVotes of those individuals. In the join version we found all the questions that had a Python tag and averaged over the UpVotes of the individuals associated with those questions. So the latter includes multiple UpVotes values from individuals who have posted multiple Python questions.)


***Challenge***: Write a query that would return the users who have answered a question with the Python tag. We've seen this challenge before, but do it now based on a subquery.

***Challenge***: How would you find all the answers associated with the user with the most upvotes?

***Challenge***: Create a frequency list of the tags used in the top 100 most answered questions. Note there is a way to do this with a JOIN and a way without a JOIN.

Finally one can use subqueries in the SELECT clause to create new variables, but we won't go into that here.

## 4.8 Challenge questions

- Use a subquery in the WHERE clause to count the tags used in the 100 most answered questions.

***Challenge***: Write 


see hard questions under window functions?

# 5) Efficient SQL queries

## 5.1) Indexes

An index is an ordering of rows based on one or more fields. DBMS use indexes to look up values quickly, either when filtering (if the index is involved in the WHERE condition) or when doing joins (if the index is involved in the JOIN condition).  So in general you want your tables to have indexes.

DBMS use indexing to provide sub-linear time lookup. Without indexes, a database needs to scan through every row sequentially, which is called linear time lookup -- if there are n rows, the lookup is $O(n)$ in computational cost. With indexes, lookup may be logarithmic -- O(log(n)) -- (if using tree-based indexes) or constant time -- O(1) -- (if using hash-based indexes). A binary tree-based search is logarithmic; at each step through the tree you can eliminate half of the possibilities. 

Here's how we create an index, with some time comparison for a simple query.

```{r, eval=SLOWEVAL}
system.time(dbGetQuery(db, "select * from questions where viewcount > 10000"))     # 2.4 seconds
system.time(dbExecute(db, "create index count_index on questions (viewcount)"))  # 5.6 seconds
system.time(dbGetQuery(db, "select * from questions where viewcount > 10000"))    # 0.9 seconds
## restore earlier state by removing index
system.time(dbExecute(db, "drop index count_index"))
```

In other contexts, an index can save huge amounts of time. So if you're working with a database and speed is important, check to see if there are indexes.

That being said, using indexes in a lookup is not always advantageous, as discussed in Section 2.6 on efficient SQL queries.

### 5.1.1) How indexes work

Indexes are often implemented using tree-based methods. For example in Postgres, b-tree indexes are used for indexes on things that have an ordering. Trees are basically like decision trees - at each node in the tree, there is a condition that sends one down the left or right branch (there might also be more than two branches. Eventually, one reaches the leaves of the tree, which have the actual values that one is looking for. Associated with each value is the address of where that row of data is stored. With a tree-based index, the time cost of b-tree lookup is logarithmic (based on the binary lookup), so it does grow with the number of elements in the table, but it does so slowly. The lookup process is that given a value (which would often be referred to as a `key`), one walks down the tree based on comparing the value to the condition at each split in the tree until one finds the elements corresponding to the value and then getting the addresses for where the desired rows are stored. 

Here's some information on how such trees are constructed and searched: http://use-the-index-luke.com/sql/anatomy/the-tree

In SQLite, indexes are implemented by creating a separate index table that maps from the value to the row index in the indexed table, allowing for fast lookup of a row. 

One downside of indexes is that creation of indexes can be very time-consuming. And if the database is updated frequently, this could be detrimental. 

## 5.2) Ideas for improving efficiency

In general, your DBMS should examine your query and try to implement it in the fastest way possible. And as discussed above, putting an indexes on your tables will often speed things up substantially, but only for certain types of queries.

Some tips for faster queries include:

 - use indexes on fields used in WHERE and JOIN clauses
    - try to avoid wildcards at the start of LIKE string comparison when you have an index on the field (as this requires looking at all of the rows)
    - similarly try to avoid using functions on indexed columns in a WHERE clause as this requires doing the calculation on all the rows in order to check the condition
 - only select the columns you really need
 - create (temporary) tables to store intermediate results that you need to query repeatedly
 - use filtering (WHERE clauses) in inner statements when you have nested subqueries
 - use LIMIT as seen in the examples here if you only need some of the rows a query returns

### 5.3) SQL query plans and EXPLAIN

You can actually examine the query plan that the system is going to use for a query using the EXPLAIN keyword. I'd suggest trying this in Postgres as the output is more interpretable than SQLite.




Now let's see what query plan is involved in a join and when using indexes. 

```{r, eval=FALSE}
dbGetQuery(db, "explain select * from questions join questions_tags on
               questions.questionid = questions_tags.questionid")
```

```
                                                                         QUERY PLAN
1                   Hash Join  (cost=744893.91..2085537.32 rows=39985376 width=118)
2                     Hash Cond: (questions_tags.questionid = questions.questionid)
3     ->  Seq Scan on questions_tags  (cost=0.00..634684.76 rows=39985376 width=16)
4                     ->  Hash  (cost=365970.96..365970.96 rows=13472796 width=102)
5         ->  Seq Scan on questions  (cost=0.00..365970.96 rows=13472796 width=102)
```

```{r, eval=FALSE}
dbGetQuery(db, "explain select * from questions join questions_tags on
               questions.questionid = questions_tags.questionid where tag like 'python'")
```

```
                                                                                                QUERY PLAN
1                                                 Gather  (cost=15339.05..899172.92 rows=687748 width=118)
2                                                                                       Workers Planned: 2
3                                        ->  Nested Loop  (cost=14339.05..829398.12 rows=286562 width=118)
4         ->  Parallel Bitmap Heap Scan on questions_tags  (cost=14338.61..252751.63 rows=286562 width=16)
5                                                                          Filter: (tag ~~ 'python'::text)
6               ->  Bitmap Index Scan on questions_tags_tag_idx  (cost=0.00..14166.68 rows=687748 width=0)
7                                                                       Index Cond: (tag = 'python'::text)
8                     ->  Index Scan using questions_pkey on questions  (cost=0.43..2.01 rows=1 width=102)
9                                                     Index Cond: (questionid = questions_tags.questionid)
```

```{r, eval=FALSE}
system.time(dbExecute(db, "create index qid_index on questions (questionid)"))  # 5.6 seconds
system.time(dbExecute(db, "create index tid_index on questions_tags (questionid)"))  # 5.6 seconds
dbGetQuery(db, "explain select * from questions join questions_tags on
               questions.questionid = questions_tags.questionid")
```

```
                                                                         QUERY PLAN
1                   Hash Join  (cost=744893.91..2085537.32 rows=39985376 width=118)
2                     Hash Cond: (questions_tags.questionid = questions.questionid)
3     ->  Seq Scan on questions_tags  (cost=0.00..634684.76 rows=39985376 width=16)
4                     ->  Hash  (cost=365970.96..365970.96 rows=13472796 width=102)
5         ->  Seq Scan on questions  (cost=0.00..365970.96 rows=13472796 width=102)
```

The "Workers Planned: 2" seems to indicate that there will be some parallelization used, even without us asking for that.

Here's additional information on interpreting what you see: [https://www.postgresql.org/docs/current/static/using-explain.html](https://www.postgresql.org/docs/current/static/using-explain.html).

The main thing to look for is to see if the query will be done by using an index or by sequential scan (i.e., looking at all the rows). 

Let's compare the query plan for doing a set operation versus a join.

```{r, eval=FALSE}
## Set operation
dbGetQuery(db, "explain select displayname, userid from
               questions Q join users U on U.userid = Q.ownerid
               join questions_tags T on Q.questionid = T.questionid
               where tag = 'r'
               intersect
               select displayname, userid from
               questions Q join users U on U.userid = Q.ownerid
               join questions_tags T on Q.questionid = T.questionid
               where tag = 'python'")
```

```{r, eval=FALSE}
## Using a join instead
dbGetQuery(db, "explain select displayname, userid from
               (questions Q1 join questions_tags T1
               on Q1.questionid = T1.questionid)
               join
               (questions Q2 join questions_tags T2
               on Q2.questionid = T2.questionid)
               on Q1.ownerid = Q2.ownerid
               join users on Q1.ownerid = users.userid
               where T1.tag = 'r' and T2.tag = 'python'")
```

- inner join versus cross join with where
- filter w/ and w/o index
- join versus intersect (see example in set operations section)

### 5.3.1) Index lookup vs. sequential scan 

Using an index is good in that can go to the data needed very quickly based on random access to the disk locations of the data of interest, but if it requires the computer to examine a large number of rows, it may not be better than sequential scan. An advantage of sequential scan is that it will make good use of the CPU cache, reading chunks of data and then accessing the individual pieces of data quickly. 

Ideally you'd do sequential scan of exactly the subset of the rows that you need, with that subset available in contiguous storage. 

## 5.4) Disk caching

On a computer there is a hierarchy of locations where data can be stored. The hierarchy has the trade-off that the locations that the CPU can access most quickly can store the least amount of data.  The hierarchy looks like this:

 -  cpu cache 
 -  main memory
 -  disk
 -  local network (data stored on other machines)
 -  general internet access

For our purposes here the key question is whether the data resides in memory or on disk, but when considering Spark and distributed systems, one gets into issues of moving data across the network between machines. 

Formally, databases are stored on disk, while R and Python store datasets in memory. This would suggest that databases will be slow to access their data but will be able to store more data than can be loaded into an R or Python session. However, databases can be quite fast due in part to disk caching by the operating system as well as careful implementation of good algorithms for database operations. For more information about disk caching see Section 2.6.5.

And conversely, R (and probably Python) have mechanisms for storing large datasets on disk in a way that they can be accessed fairly quickly.

You might think that database queries will generally be slow (and slower than in-memory manipulation such as in R or Python when all the data can fit in memory) because the database stores the data on disk. However, as mentioned earlier the operating system will generally cache files/data in memory when it reads from disk. Then if that information is still in memory the next time it is needed, it will be much faster to access it the second time around. Other processes might need memory and 'invalidate' the cache, but often once the data is read once, the database will be able to do queries quite quickly. This also means that even if you're using a database, you can benefit from a machine with a lot of memory if you have a large database (ideally a machine with rather more RAM than the size of the table(s) you'll be accessing). 

Given this, it generally won't be helpful to force your database to reside in memory (e.g., using `:memory:` for SQLite or putting the database on a RAM disk). 


## 5.5) Parallelization and partitioning

To speed up your work, one might try to split up one's queries into multiple queries that you run in parallel. However, you're likely to have problems with parallel queries from a single R or Python session.

However, multiple queries to the same database from separate R or Python sessions will generally run fine but can compete for access to disk/memory. That said, in some basic experiments, the slowdown was moderate, so one may be able to parallelize across processes in a manual fashion.

As of version 9.6 of Postgres, there is some capability for doing parallel queries: 
[https://www.postgresql.org/docs/current/static/parallel-query.html](https://www.postgresql.org/docs/current/static/parallel-query.html).

Finally Postgres supports partitioning tables. Generally one would divide a large table into smaller tables based on unique values of a key. For example if your data had timetamps, you could partition into subtables for each month or each year. This would allow faster queries when considering data that reside on one or a small number of partitions and could also ease manual implementation of parallelization.  Here's some information:  https://www.postgresql.org/docs/current/static/ddl-partitioning.html.


# 6) Window functions

[Window functions](https://www.postgresql.org/docs/current/functions-window.html) provide the ability to perform calculations across sets of rows that are related to the current query row.

Comments:

 - The result of applying a window function is the same number of rows as the input, even though the functionality is similar to `group by`. Hint: think about the result of `group by` + `mutate` in dplyr in R.
 - One can apply a window function within groups or across the whole table.
 - The functions one can apply include standard aggregation functions such as `avg` and `count` as well as non-standard functions (specific to using window functions) such as `rank` and `cume_dist`.
 - Unless you're simply grouping into categories, you'll generally need to order the rows for the window function to make sense.

The syntax is a bit involved, so let's see with a range of examples:

 - Aggregate within groups but with one output value per input row
 
```{r, eval=TRUE}
## Total number of questions for each owner
dbGetQuery(db, "select *,
                count() over (partition by ownerid) as n
                from questions order by creationdate limit 5")
```

 - Compute cumulative calculations; note the need for the 'order by'
```{r, eval=TRUE}
## Rank (based on ordering by creationdate) of questions by owner
dbGetQuery(db, "select *,
                rank() over (partition by ownerid order by creationdate) as rank
                from questions limit 5")
dbGetQuery(db, "select *,
                rank() over (partition by ownerid order by creationdate) as rank
                from questions order by ownerid desc limit 10")
```

 - Do a lagged analysis
```{r, eval=TRUE}
## Get previous value (based on ordering by creationdate) by owner
dbGetQuery(db, "select ownerid, creationdate,
                lag(creationdate, 1) over
                (partition by ownerid order by creationdate)
                as previous_date
                from questions order by ownerid desc limit 5")
```
So one could now calculate the difference between the previous and current date to analyze the time gaps between users posting questions.

 - Do an analysis within an arbitrary window of rows based on the values in one of the columns
 
```{r, eval=TRUE}
## Summarize questions within 5 days of current question 
dbGetQuery(db, "select ownerid, creationdate,
                count() over
                (partition by ownerid order by julianday(creationdate)
                range between 5 preceding and 5 following)
                as n_window
                from questions where ownerid is not null limit 30")
```
There the '5 preceding' and '5 following' mean to include all rows within each ownerid
that are within 5 Julian days (based on 'creationdate') of each row. 

So one could now analyze bursts of activity.

One can also choose a fixed number of rows by replacing 'range' with 'rows'. The ROWS and RANGE syntax allow one to specify the *window frame* in more flexible ways than simply the categories of a categorical variable.

So the syntax of a window function will generally have these elements:

 - a call to some function
 - OVER
 - PARTITION BY (optional)
 - ORDER BY (optional)
 - RANGE or ROW (optional)
 - AS (optional)


You can also name window functions, which comes in handy if you want multiple functions applied to the same window:


```{r, eval=TRUE}
dbGetQuery(db, "select ownerid, creationdate,
                lag(creationdate, 1) over w as lag1,
                lag(creationdate, 2) over w as lag2
                from questions where ownerid is not null
                window w as (partition by ownerid order by creationdate)
                order by ownerid limit 5")
```

What does that query do?

***Challenge***: Use a window function to compute the average viewcount for each ownerid for the 10 questions preceding each question.

## 6.1) Putting it all together to do complicated queries

Here are some real-world style questions one might try to create queries to answer. The context would be if you have data on user sessions on a website or data on messages between users. 

1) Given a table of user sessions with the format
```
date | session_id | user_id | session_time
```
calculate the distribution of the average daily
total session time in the last month. I.e., you want to get each user's daily average and then find the distribution over users. The output should be something
like:
```
minutes_per_day' | number_of_users
```

2) Consider a table of messages of the form
```
sender_id | receiver_id | message_id
```
For each user, find the three users they message the most.

3) Suppose you have are running an online experiment and have a table on
the experimental design
```
user_id | test_group | date_first_exposed
```
Suppose you also have a messages table that indicates if each message
was sent on web or mobile:
```
date | sender_id | receiver_id | message_id | interface (web or mobile)
```
What is the average (over users) in the average number of messages sent per day for each test group
if you look at the users who have sent messages only on mobile in the last month.

# 7) Database management and command-line operation

We'll illustrate some basic database management using a different example dataset. This is some data on webtraffic to Wikipedia pages. Note that the input file used here involved some pre-processing relative to the data you get the directly from the Wikistats dataset available through Amazon Web Services (AWS) because in the data posted on AWS, the datetime information is part of the filename.

## 7.1) SQLite

#### Setting up a database and using the SQLite command line

With SQLite you don't need to deal with all the permissions and administrative overhead because an SQLite database is simply a file that you can access without a password or connecting to a database server process.

To start the SQLite interpreter in Linux, either operating on or creating a database named `wikistats.db`:

```{bash}
sqlite3 wikistats.db
```

Here's the syntax to create an (empty) table:

```
create table webtraffic
(date char(8), hour char(6), site varchar, page varchar, count integer, size double precision);
.quit
```

### 7.1.1) Populating a table

Here's an example of reading from multiple files into SQLite using the command line.
We create a file `import.sql` that has the configuration for the import:

```
.separator " "
.import /dev/stdin webtraffic
```

Then we can iterate through our files from the UNIX shell, piping the output of gzip to the `sqlite3` interpreter:

```{bash}
for file in $(ls part*gz); do
    echo "copying $file"
    gzip -cd $file | sqlite3 wikistats.db '.read import.sql'
done
```

### 7.1.2) Data cleaning

The problem in this example with importing into SQLite is the presence of double quote (") characters that are not meant to delineate strings but are actually part of a field. In this case probably the easiest thing is simply to strip out those quotes from UNIX. Here we use `sed` to search and replace to create versions of the input files that don't have the quotes.

```{bash}
for file in $(ls *gz); do
    gzip -cd ${file} | sed  "s/\"//g" | gzip -c > wikistats-cleaned/${file}
done
```

If you want to read the data into SQLite yourself, you *will* need to do something about the quotes; I haven't stripped them out of the files.

## 7.2) PostgreSQL

- create db on scf or in container
- show access to db from another machine
- show port forwarding?

### 7.2.1) Setting up a database and using the Postgres command line

First make sure Postgres is installed on your machine.

On Ubuntu, you can install Postgres easily via `apt-get`:
```
sudo apt-get install postgresql postgresql-contrib
```

Next we'll see how to set up a database. You'll generally need to operate as the `postgres` user for these sorts of manipulations. Of course if you're just a user accessing an existing database and existing tables, you don't need to worry about this.

```{bash}
sudo -u postgres -i  # become the postgres user
psql  # start postgres interpreter
```

Now from within the Postgres interpreter, you can create a database, tables within the database, and authenticate users to do things with those tables. 

In this case we'll be explicit about where on the disk filesystem the database is stored, but you probably only need to worry about this if you are creating a large database. 

```
create database wikistats;
create user paciorek with password 'test';
grant all privileges on database wikistats to paciorek;
```

PostgreSQL and other DBMS (not SQLite) allow various kinds of control over permissions to access and modify databases and tables as well.
It can get a bit involved because the administrator has fine-grained control over what each user can do/access.


Now let's create a table in the database, after first connecting to the specific database so as to operate on it. 

```
\connect wikistats
create table webtraffic (date char(8), hour char(6), site varchar, page varchar,
       count integer, size double precision);
grant all privileges on table webtraffic to paciorek;
\quit
```

Note the use of `\` to do administrative tasks (as opposed to executing SQL syntax), and the use of `;` to end each statement. Without the semicolon, Postgres will return without doing anything.

If you want control over where the database is stored (you probably only need to worry about this if you are creating a large database), you can do things like this:

```
show data_directory;
create tablespace dbspace location '/var/tmp/pg';
create database wikistats tablespace dbspace;
create user paciorek with password 'test';
grant all privileges on database wikistats to paciorek;
```

### 7.2.2) Populating a table 

Here's an example of importing a single file into Postgres from within the psql interpreter running as the special postgres user. In this case we have space-delimited text files. You can obtain the file `part-00000` as discussed in the introduction (you'll need to run `gunzip part-00000.gz` first).

```
\connect wikistats
copy webtraffic from 'part-00000' delimiter ' ';
```

If one had CSV files, one could do the following

```
copy webtraffic from 'part-00000' csv;
```

To actually handle the Wikistats input files, we need to deal with backslash characters occurring at the end of text for a given column in some rows. Ordinarily in standard Postgres 'text' format (different from Postgres 'csv' format), a backslash is used to 'quote' characters that would usually be treated as row or column delimiters (i.e., preceding such a character by a backslash means it is treated as a character that is part of the field). But we just want the backslash treated as a character itself. So we need to tell Postgres not to treat a backslash as the quoting character. To do that we specify the `quote` character. However, the quote keyword is only provided when importing 'csv' format. In 'csv' format the double-quote character is by default treated as delineating the beginning and end of text in a field, but the Wikistats files have double-quotes as part of the fields. So we need to set the quote character as neither a double-quote nor a backslash. The following syntax does that by specifying that the quote character is a character (\b) that never actually appears in the file. The 'e' part is so that Postgres treats \b as a single character, i.e., 'escaping' the backslash, and the 'csv' is because the quote keyword only works with the csv format, but note that by setting the delimiter to a space, it's not really a CSV file! 

```
copy webtraffic from 'part-00000' delimiter ' ' quote e'\b' csv;
```

Often you'll need to load data from a large number of possibly zipped text files. As an example of how you would load data in a case like that, here's some shell scripting that will iterate through multiple (gzipped) input files of Wikistats data, running as the regular user:

```{bash}
export PGPASSWORD=test  # set password via UNIX environment variable
for file in $(ls part*gz); do  # loop thru files whose names start with 'part' and end with 'gz'
  echo "copying $file"
  ## unzip and then pass by UNIX pipe to psql run in non-interactive mode
  gzip -cd $file |
    psql -d wikistats -h localhost -U paciorek -p 5432 -c "\copy webtraffic from stdin delimiter ' ' quote e'\b' csv"
done
```

Using `\copy` as above invokes the psql `copy` command (`copy` would invoke the standard SQL `copy` command), which allows one to operate as a regular user and to use relative paths. In turn `\copy` invokes `copy` in a specific way. 


### 7.2.3) Data cleaning

One complication is that often the input files will have anomalies in them. Examples include missing columns for some rows, individual elements in a column that are not of the correct type (e.g., a string in a numeric column), and characters that can't be handled. In the Wikistats data case, one issue was lines without the full set of columns and another was the presence of a backslash character at the end of the text for a column.

With large amounts of data or many files, this can be a hassle to deal with. UNIX shell commands can sometimes be quite helpful, including use of sed and awk. Or one might preprocess files in chunks using Python. 

For example the following shell scripting loop over Wikistats files ensures each row has 6 fields/columns by pulling out only rows with the full set of columns. I used this to process the input files before copying into Postgres as done above. Actually there was even more preprocessing because in the form of the data available from Amazon's storage service, the date/time information was part of the filename and not part of the data files. 

```{bash}
for file in $(ls *gz); do
    gzip -cd $file | grep "^.* .* .* .* .* .*$" | gzip -c > ../wikistats-fulllines/$file
done
```

Note that this restriction to rows with a full set of fields has already been done in the data files I provide to you.


# 8) References

In addition to various material found online, including various software manuals and vignettes, much of the SQL material was based on the following two sources:

 - The Stanford online [Introduction to Databases course](http://cs.stanford.edu/people/widom/DB-mooc.html) (see also the [mini-courses version of the course](https://lagunita.stanford.edu/courses/DB/2014/SelfPaced/about)).
 - Harrison Dekker's materials from a [Statistics short course](https://github.com/uc-data-services/sql-workshop-2016) he taught in January 2016.

